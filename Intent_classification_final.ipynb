{
  "cells": [
    {
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "<a href=\"https://colab.research.google.com/github/Dark-Sied/Intent_Classification/blob/master/Intent_classification_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**INTENT CLASSIFICATION USING NEURAL NETWORKS**"
    },
    {
      "metadata": {
        "id": "a_WypuUXi92e",
        "colab_type": "code",
        "outputId": "133d026e-4236-4ff6-f21d-739bfb9640db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.lancaster import LancasterStemmer\nimport nltk\nimport re\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\nfrom keras.callbacks import ModelCheckpoint\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  'Matplotlib is building the font cache using fc-list. '\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This function fetches a dataset of sentences and their intents."
    },
    {
      "metadata": {
        "id": "LE6wywJrN2ih",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def load_dataset(filename):\n  df = pd.read_csv(filename, encoding = \"latin1\", names = [\"Sentence\", \"Intent\"])\n  print(df.head())\n  intent = df[\"Intent\"]\n  unique_intent = list(set(intent))\n  sentences = list(df[\"Sentence\"])\n  \n  return (intent, unique_intent, sentences)\n  \n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's load the dataset and print the first 5 rows."
    },
    {
      "metadata": {
        "id": "tF0FQA7gjOCX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c609b42a-05da-49f5-8d11-bd670210f635",
        "trusted": true
      },
      "cell_type": "code",
      "source": "intent, unique_intent, sentences = load_dataset(\"Dataset.csv\")",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                Sentence          Intent\n0       Need help pleese  commonQ.assist\n1              Need help  commonQ.assist\n2       I need some info  commonQ.assist\n3      Will you help me?  commonQ.assist\n4  What else can you do?  commonQ.assist\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now create a \"tokenizer\" object which separates sentences into words using a 'filter' list."
    },
    {
      "metadata": {
        "id": "SJCQ_YhBJW7t",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n  #print(words)\n  tokenizer = Tokenizer(filters = filters)\n  tokenizer.fit_on_texts(words)\n  return tokenizer",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's now tokenize the sentences and find out the vocabulary size. Let's also fix a maximum length for sentences. This max_length will be used to 'pad' the short sentences later on."
    },
    {
      "metadata": {
        "id": "JWjxPGsZZJNX",
        "colab_type": "code",
        "outputId": "b02c8f6b-d0df-4e90-fa3a-2ff730c88300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "word_tokenizer = create_tokenizer(sentences)\nvocab_size = len(word_tokenizer.word_index) + 1\nmax_length = 30\nprint(\"Vocab Size = \",vocab_size)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Vocab Size =  494\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, we convert each sentence (i.e. sequence of words) into a sequence of indices. This process is sometimes called encoding. "
    },
    {
      "metadata": {
        "id": "D0TXu2xsR8jq",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def encoding_doc(tokenizer, words):\n  return(tokenizer.texts_to_sequences(words))",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dE92Hk1Va--H",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "encoded_doc = encoding_doc(word_tokenizer, sentences)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Pad all the short sequences to max_length, to get the final input matrix."
    },
    {
      "metadata": {
        "id": "fyOzLEboc4LZ",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def padding_doc(encoded_doc, max_length):\n  return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WdejoJrlc-tc",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "padded_doc = padding_doc(encoded_doc, max_length)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's see what the first 5 inputs, after padding, look like."
    },
    {
      "metadata": {
        "id": "gDgTCS2KdI2p",
        "colab_type": "code",
        "outputId": "ac5332cd-0a0f-4311-8db4-22df92728d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "padded_doc[:5]",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "array([[ 24,  77, 332,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0],\n       [ 24,  77,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0],\n       [  2,  24, 198, 181,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0],\n       [ 51,  10,  77,  16,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0],\n       [  9, 268,   4,  10,  30,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0]], dtype=int32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The size of input can be found as follows. The first value is the number of data samples, and the second is the dimension of each sample."
    },
    {
      "metadata": {
        "id": "3eaSIDi0dNf1",
        "colab_type": "code",
        "outputId": "4ab6b6dd-ffa4-4061-9e9d-7a01decfa837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Shape of padded docs = \",padded_doc.shape)",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Shape of padded docs =  (1113, 30)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X0rXzenSpgFR",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "#tokenizer with filter changed\noutput_tokenizer = create_tokenizer(unique_intent, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following command gives us the different intents present in the dataset"
    },
    {
      "metadata": {
        "id": "yNHQtkszskxr",
        "colab_type": "code",
        "outputId": "f5babc01-89e3-4392-e8e6-c9f257de3d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "output_tokenizer.word_index",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "{'commonq.assist': 14,\n 'commonq.bot': 8,\n 'commonq.how': 2,\n 'commonq.just_details': 19,\n 'commonq.name': 17,\n 'commonq.not_giving': 6,\n 'commonq.query': 10,\n 'commonq.wait': 20,\n 'contact.contact': 21,\n 'faq.aadhaar_missing': 4,\n 'faq.address_proof': 15,\n 'faq.application_process': 1,\n 'faq.apply_register': 5,\n 'faq.approval_time': 3,\n 'faq.bad_service': 7,\n 'faq.banking_option_missing': 16,\n 'faq.biz_category_missing': 9,\n 'faq.biz_new': 12,\n 'faq.biz_simpler': 13,\n 'faq.borrow_limit': 11,\n 'faq.borrow_use': 18}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's encode the outputs too. This means we assign each unique class an index."
    },
    {
      "metadata": {
        "id": "7OOx9qdBto1-",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "encoded_output = encoding_doc(output_tokenizer, intent)",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_5Lv5PiyG-z",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dpM86WrVQlx5",
        "colab_type": "code",
        "outputId": "71ff52a6-b3d0-4b5c-850d-5dc0a56c8aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "encoded_output.shape",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "(1113, 1)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "rD3QN-RPzfet",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def one_hot(encode):\n  o = OneHotEncoder(sparse = False)\n  return(o.fit_transform(encode))",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z6wP_Xed7RNR",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "output_one_hot = one_hot(encoded_output)",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following commands shows that there are 1113 data samples, and 21 classes"
    },
    {
      "metadata": {
        "id": "A6HVslLTHgOM",
        "colab_type": "code",
        "outputId": "752962df-02d8-409b-fb8f-adb06227161d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "output_one_hot.shape",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "(1113, 21)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "EqABUESD7xi9",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Split the data into training set and validation set"
    },
    {
      "metadata": {
        "id": "h8P4HTz6A4E-",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)\n",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7E0uhC2OCtTx",
        "colab_type": "code",
        "outputId": "6ce0e215-aa3f-43f1-ba5a-0b584b25a35c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\nprint(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Shape of train_X = (890, 30) and train_Y = (890, 21)\nShape of val_X = (223, 30) and val_Y = (223, 21)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, we create the neural network. It contains an embedding layer, which converts each input word to a vector of specified size (128 in this case). This is followed by a recurrent neural network (LSTM), followed by a fully connected layer (Dense), a dropout layer, and another fully connected layer."
    },
    {
      "metadata": {
        "id": "e5BU_x74DNEb",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_model(vocab_size, max_length):\n  model = Sequential()\n  model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n  model.add(Bidirectional(LSTM(128)))\n#   model.add(LSTM(128))\n  model.add(Dense(32, activation = \"relu\"))\n  model.add(Dropout(0.5))\n  model.add(Dense(21, activation = \"softmax\"))\n  \n  return model",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We define the loss function. Categorical crossentropy is essential negative log likelihood of the data."
    },
    {
      "metadata": {
        "id": "f-NvE0P7MFCe",
        "colab_type": "code",
        "outputId": "8f07056b-579e-4c15-e1af-bdfa8f681e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = create_model(vocab_size, max_length)\n\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\nmodel.summary()",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 30, 128)           63232     \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 256)               263168    \n_________________________________________________________________\ndense_3 (Dense)              (None, 32)                8224      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 21)                693       \n=================================================================\nTotal params: 335,317\nTrainable params: 272,085\nNon-trainable params: 63,232\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_r-dxm2sMQ-d",
        "colab_type": "code",
        "outputId": "3c37b4f8-fc4e-4c82-ab46-2aa1d8b47ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6834
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "filename = 'model.h5'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\nhist = model.fit(train_X, train_Y, epochs = 10, batch_size = 64, validation_data = (val_X, val_Y), callbacks = [checkpoint])",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 890 samples, validate on 223 samples\nEpoch 1/10\n832/890 [===========================>..] - ETA: 1s - loss: 3.0187 - acc: 0.0637Epoch 00000: val_loss improved from inf to 2.93226, saving model to model.h5\n890/890 [==============================] - 34s - loss: 3.0128 - acc: 0.0674 - val_loss: 2.9323 - val_acc: 0.1031\nEpoch 2/10\n832/890 [===========================>..] - ETA: 1s - loss: 2.9587 - acc: 0.0889Epoch 00001: val_loss improved from 2.93226 to 2.88628, saving model to model.h5\n890/890 [==============================] - 22s - loss: 2.9569 - acc: 0.0888 - val_loss: 2.8863 - val_acc: 0.2018\nEpoch 3/10\n832/890 [===========================>..] - ETA: 1s - loss: 2.8892 - acc: 0.1202Epoch 00002: val_loss improved from 2.88628 to 2.73986, saving model to model.h5\n890/890 [==============================] - 22s - loss: 2.8868 - acc: 0.1213 - val_loss: 2.7399 - val_acc: 0.2152\nEpoch 4/10\n832/890 [===========================>..] - ETA: 1s - loss: 2.8160 - acc: 0.1346Epoch 00003: val_loss improved from 2.73986 to 2.66046, saving model to model.h5\n890/890 [==============================] - 23s - loss: 2.8067 - acc: 0.1371 - val_loss: 2.6605 - val_acc: 0.2063\nEpoch 5/10\n832/890 [===========================>..] - ETA: 1s - loss: 2.7658 - acc: 0.1550Epoch 00004: val_loss did not improve\n890/890 [==============================] - 22s - loss: 2.7681 - acc: 0.1584 - val_loss: 2.7223 - val_acc: 0.2197\nEpoch 6/10\n832/890 [===========================>..] - ETA: 1s - loss: 2.7441 - acc: 0.1671Epoch 00005: val_loss improved from 2.66046 to 2.57447, saving model to model.h5\n890/890 [==============================] - 25s - loss: 2.7523 - acc: 0.1629 - val_loss: 2.5745 - val_acc: 0.2063\nEpoch 7/10\n832/890 [===========================>..] - ETA: 1s - loss: 2.7052 - acc: 0.1550Epoch 00006: val_loss did not improve\n890/890 [==============================] - 22s - loss: 2.6993 - acc: 0.1551 - val_loss: 2.5771 - val_acc: 0.2152\nEpoch 8/10\n832/890 [===========================>..] - ETA: 1s - loss: 2.6495 - acc: 0.1755Epoch 00007: val_loss improved from 2.57447 to 2.50490, saving model to model.h5\n890/890 [==============================] - 28s - loss: 2.6485 - acc: 0.1764 - val_loss: 2.5049 - val_acc: 0.2063\nEpoch 9/10\n832/890 [===========================>..] - ETA: 1s - loss: 2.6649 - acc: 0.1526Epoch 00008: val_loss improved from 2.50490 to 2.49929, saving model to model.h5\n890/890 [==============================] - 22s - loss: 2.6564 - acc: 0.1539 - val_loss: 2.4993 - val_acc: 0.2197\nEpoch 10/10\n832/890 [===========================>..] - ETA: 1s - loss: 2.5920 - acc: 0.1719Epoch 00009: val_loss improved from 2.49929 to 2.41476, saving model to model.h5\n890/890 [==============================] - 21s - loss: 2.5896 - acc: 0.1719 - val_loss: 2.4148 - val_acc: 0.2422\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To check how well our model does, we create the following two functions. Then we invoke the model on a test input of our choice."
    },
    {
      "metadata": {
        "id": "qSTEzrlzcuya",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def predictions(text):\n  clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n  test_word = word_tokenize(clean)\n  test_word = [w.lower() for w in test_word]\n  test_ls = word_tokenizer.texts_to_sequences(test_word)\n  print(test_word)\n  #Check for unknown words\n  if [] in test_ls:\n    test_ls = list(filter(None, test_ls))\n    \n  test_ls = np.array(test_ls).reshape(1, len(test_ls))\n \n  x = padding_doc(test_ls, max_length)\n  \n  pred = model.predict_proba(x)\n  \n  \n  return pred\n\n\n  ",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P1ddofshmdzK",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_final_output(pred, classes):\n  predictions = pred[0]\n \n  classes = np.array(classes)\n  ids = np.argsort(-predictions)\n  classes = classes[ids]\n  predictions = -np.sort(-predictions)\n \n  for i in range(pred.shape[1]):\n    print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n\n",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "id": "23VpGuihMdEU",
        "colab_type": "code",
        "outputId": "cd36c932-0fb0-4166-92ae-546a7676e645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "text = \"Can you help me?\"\npred = predictions(text)\nget_final_output(pred, unique_intent)",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['can', 'you', 'help', 'me']\n1/1 [==============================] - 0s\ncontact.contact has confidence = 0.15496406\nfaq.application_process has confidence = 0.11929979\nfaq.borrow_use has confidence = 0.07631553\nfaq.apply_register has confidence = 0.06454459\nfaq.biz_new has confidence = 0.062133495\nfaq.biz_simpler has confidence = 0.0520627\nfaq.address_proof has confidence = 0.04624241\ncommonQ.assist has confidence = 0.044755496\nfaq.aadhaar_missing has confidence = 0.043643948\nfaq.approval_time has confidence = 0.041520894\ncommonQ.name has confidence = 0.036536757\nfaq.borrow_limit has confidence = 0.03383332\nfaq.banking_option_missing has confidence = 0.03310464\ncommonQ.how has confidence = 0.03139223\nfaq.biz_category_missing has confidence = 0.030802295\ncommonQ.bot has confidence = 0.026777714\ncommonQ.just_details has confidence = 0.024570618\nfaq.bad_service has confidence = 0.021721687\ncommonQ.wait has confidence = 0.019781144\ncommonQ.query has confidence = 0.01942338\ncommonQ.not_giving has confidence = 0.01657323\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bKUBDT36IHKO",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Intent_classification_final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "TPU",
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}